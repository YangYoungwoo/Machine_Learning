# 경북대학교 SW_2020 평가용 repositories  

[summary.md](https://github.com/YangYoungwoo/sw2020/blob/main/summary.md)

## 머신 러닝이란?
>1. machine learning 점점 시간이 지날수록 컴퓨터가 스스로 발전한다.
	데이터를 통해 규칙을 스스로 찾아서 조율을 하여 데이터를 계속 추가하여 학습
	특정 작업을 하는데 있어서 경험을 통해 작업의 성능을 향상시킨다.


>2. machine learning이 사용 가능한 이유
	데이터가 충분히 많아져서 사용 가능 – 개인 휴대폰이나 컴퓨터를 사용하여 데이터  급증 
	컴퓨터의 성능이 좋아져서 사용 가능 – 프로그램을 학습시키는데 시간이 절약
	활용성이 증명되어 사용가능 – 사용자의 데이터가 많아질수록 맞춤형으로 추천 가능
	더 좋은 제품과 더 많은 수익 창출을 할 수 있는 기반이 된다.
	데이터를 모으는데 더 쉬워졌다.   


>3. 빅 데이터 – 많은 양의 데이터를 효율적으로 처리, 저장, 분석
   인공지능 – 컴퓨터 프로그램이 인간처럼 생가하고 행동하도록 처리
   딥 러닝 – 머신 러닝의 기법 중 하나
  인공지능>머신 러닝> 딥 러닝     -> 일부는 빅 데이터를 사용   

>4. 머신 러닝 학습의 유형
  지도학습 – 답이 있고 답을 맞히는 게 학습의 목적
	분류 – 두 가지 옵션 중 하나를 고르는 것
	회귀 – 무수히 많은 연속적인 값 중 고르는 것
     학습에 대한 답을 꼭 알려줘야 됨
     직관적이고 더 많이 사용
  비지도학습 – 답이 없고 프로그램이 알아서 답을 맞히는 게 학습의 목적
	정답이 없고 비슷한 기준으로 묶어서
  강화학습


>5. k-NN(nearest neighber) 알고리즘
	근처에 있는 것 k개를 비교해서 지도학습으로 이용


>6. 머신 러닝의 수학
	컴퓨터 과학 + 수학
	코드만 사용해도 가능하기 때문에 알고리즘 선택하면 됨
	선형대수 – 행렬(계산을 효율적으로 할 수 있음)
	미적분 – 최적화를 할 때 사용
	통계 – 데이터를 가지고 흐름과 특징을 파악하기 위해 사용
	확률 – 가능성을 확인하기 위해 사용



### 선형대수학 – 일차식과 일차함수를 공부하는 학문
>1. 일차식과 일차 함수
	일차식 – 가장 높은 차수가 1인 다항식
	일차 함수 – 일차식과 같은 함수



>2. 일차 함수 표기법
	f(x, y) = 3x+y+3 -> x0, x1, x2로 사용하고 계수는 a0, a1 상수는 b로 사용



>3. 행렬과 벡터
	행렬(matrix) - 가로->행 세로->열 행렬안의 요소->원소 A21로 표현
	벡터(vector) - 행 또는 열이 하나만 있는 것 원소의 개수로 표현 a1로 표현
		기본인 경우 열 벡터 소문자로 표현


>4. numpy로 행렬 사용하기
	import numpy as np
	A = np.array{[
		[1, -1, 2],
		[3, 2, 2]
	]}
	np.random.rand(3, 5) -> 0에서 1사이 값
	np.zeros((2, 4)) -> 0으로 모든 값


>5. 행렬 연산
	덧셈 – 같은 위치의 값을 더하기,  차원이 같아야 함
	스칼라 곱 – 행렬과 수를 곱하는 것
	두 행렬의 곱
		내적 곱 – 행*열, 앞의 행과 뒤의 열의 길이가 같아야 함 앞행*뒷열행렬됨
			  AB != BA 교환법칙 성립 안함
		요소별 곱하기 – A○B로 사용 각 위치끼리 서로 곱하기 a[1][1]과 b[1][1]곱해서 결과는 c[1][1]로 표시
		외적 곱


>6. numpy로 행렬 연산하기
	A+B – 더하기 
	5*A – 스칼라 곱
	np.dot(A, B), A@B – 행렬*행렬
	다양한 식 같이 사용 가능 연산자 우선순위로 실행


>7. 전치 행렬, 단위 행렬, 역행렬 노드
	전치 행렬 – A^T -> 행렬의 열과 행을 바꿈 첫 행이 첫 열이 됨
	단위 행렬 – I로 표시, 대각선 원소가 1 나머지는 0, 단위 행렬과 곱하면 원래의 값 그대로 나옴
	역행렬 – A^(-1) -> A와 곱하면 I가 나옴, 없을 때도 있음


>8. numpy로 전치, 단위, 역행렬 사용하기
	전치 – np.transpose(A) or A.T
	단위 – np.identity(A)
	역행렬 – np.linalg.pinv(A) 역행렬이 없을 경우 가장 가까운 함수로 나타냄


>9. 선형대수학과 행렬/벡터
	선형 시스템을 행렬로 표시 가능 
	복잡한 선형 시스템을 쉽게 표시 가능


>10. 선형대수학이 머신 러닝에 필요한 이유
	데이터를 일차식에 사용하는 경우가 많다
	행렬로 이용하면 정돈된 형태로 효율적으로 계산 가능
	선형대수학이 행렬과 일차식을 사용하기 때문에 알고 있어야 됨



---




   ### 미적분

>1. 함수
	input을 받아 function을 사용하여 output을 추출한다.
	하나의 input에 하나의 output만 가지는 것	
<br>

>2. 그래프
	수학식을 시각적으로 표현하는 것
	x축과 y축을 사용하여 함수에 들어가는 모든 점을 찍는 것-> 직선이 된다. 일차함수
							     -> 아치 모양, 이차함수
	함수의 특징을 파악하기 쉬움
<br>

>3. 평균변화율
	그래프의 기울기로 x가 변할 때 y가 얼마나 빨리 변하는지 확인 가능
	일차 함수는 기울기가 항상 일정함
	이차 함수의 경우 특정 위치마다 기울기가 다르므로 선을 그어 순간변화율을 이용하여 계산
	두 점의 y축 사이의 거리/ 두 점의 x축 사이의 거리
<br>

>4. 순간변화율
	특정 위치와 거리차가 0에 가까운 점을 이용하여 두 점으로 평균 변화율을 계산
	lim를 사용 후 h에 0 대입
<br>

>5. 미분
	프라임으로 읽는다. f’(x)는 기본 함수의 미분이다.  d/dx*f(x)
	기울기가 음수면 x가 커질수록 y가 작아짐 
	기울기가 0에 가까우면 평평해지고 멀어지면 가파름
	기울기가 양수면 x가 커질수록 y가 커짐 
	그래프가 해당 지점에서 얼마나 기울어져 있는지, 어떤 방향으로 가야 가파르게 올라갈 수 있는지 확인 가능
<br>

>6. 극소점, 극대점
	기울기가 0인 지점
	극소점 – 아래로 볼록 올라와서 음수에서 양수로 전환하는 점 여러 개의 극소점이 있음 -> 가장 작은 점은 최소점 이라고 함
	극대점 – 위로 볼록 올라와서 양수에서 음수로 전환하는 점 -> 가장 큰 점 최대점
	안장점 – 기울기가 평평해졌다가 다시 가팔라지는 경우

<br>

>11. 고차원에서 미분
	편미분 – 함수를 변수 하나에 대해서만 미분 x 또는 y를 고정시키고 나머지를 미분
<br>

>12. 머신 러닝에 미분이 필요한 이유
	경험을 통해 특정 작업에 대한 성능이 좋아지도록 하는 프로그램이다.
	함수의 극소점이나 극대점을 찾아 성능의 최적화를 찾기 위해 순간변화율 사용




---

### 기본 지도 학습 알고리즘

>1. 선형 회귀
	데이터를 프로그램에 학습을 시켜 가장 적절한 선(최적선)을 만들고 다른 데이터를 입력을 받아 비슷한 위치의 결과를 출력하도록 하는 것
	지도 학습에 해당이 된다. 수많은 답을 알려주면서 지도를 한다.
		회귀에 속한다. 연속적인 값을 비교하기 때문
	목표 변수(y) – 맞추려고 하는 값
	입력 변수(x) – 맞추는데 사용하는 값 x^(1)로 표시
	학습 데이터(m) - 미리 입력을 해놓은 데이터들


>2. 가설 함수
	실행을 하며 최적선을 찾는 함수
	h로 표시
	세타를 사용 상수의 경우 세타0으로 표시


>3. 평균 제곱 오차(MSE)
	가설 함수 평가법
	데이터와 가설 함수가 평균적으로 얼마나 차이가 나는지 제곱으로 오차를 확인
	오차 값들을 모두 제곱을 한 뒤 모두 더해서 총 개수로 나누기
	제곱을 해서 모든 오차를 양수로 만들 수 있고 오차의 크기를 더 부각시킬 수 있다.


>4. 손실 함수
	가설 함수의 성능을 평가하는 함수
	함수가 작으면 가설 함수가 데이터가 잘 맞다.
	J를 사용


>5. 경사 하강법
	임의의 점을 지정하여 기울기를 확인하여 극소점을 찾아가는 법
	경사의 방향과 크기로 비교를 하며 점을 찾기
	경사를 줄이는 방법을 사용하면 극소점에 도달한다.
	알파가 너무 큰 경우 극소점에서 멀어질 수도 있다.
	알파가 너무 작은 경우 극소점까지 가는 시간이 오래 걸린다.
	1에서 0사이의 숫자로 실험을 해보며 가장 손실잉 잘 줄어드는 학습률을 선택한다.


>6. 모델 평가하기
	평균 제곱근 오차 사용 – 제곱이 된 것을 다시 원래의 형태로 돌리기 위해
	학습을 위한 데이터(training set)와 평가를 위한 데이터(test set)를 나눈다.

>7. scikit-learn
	from sklearn.dataset import load_boston
	import pandas as pd
	load_boston().DESCR(description줄임말)
	.feature_names
	.data
	.data.shape
	.target
	pd.dataFrame()
	data set 나누기
	선형 회귀하기


---

### 다중 선형 회귀
#### 1. 다중 선형 회귀
    ->시각적으로 표현하기 힘듦
<br>

    >**표현법**
        입력법 – 속석(feature) x1, x2등으로 표현
        목표변수는 무조건 하나 y로 나타냄
        x^(i)j - i번째 데이터의 j번째 속성

    >**가설 함수**
        입력을 받아 결과를 예측하는 것
        벡터를 사용하여 간결하게 표현가능

    >**경사 하강법**
        손실 함수로 가설 함수를 평가함
        손실 함수가 크면 가설 함수가 데이터에 잘 안 맞는다고 판단
        손실을 가장 빠르게 줄이는 방향으로 세타값을 바꾸는 것
        선형 회귀와 같이 계산
        세타 양이 많아지고 동일
        특징
            적합한 학습율을 찾아야 된다.
            반복문을 사용해야 한다.
            입력 변수의 개수가 커도 효율적으로 계산이 가능하다.

    >**정규 방정식**
        미분을 하여 기울기가 0이 되는 곳으로 극소점을 찾기
        특징
            학습율을 정할 필요가 없다.
            한 단계로 계산을 끝낼 수 있다.
            입력 변수의 개수가 커지면 비효율적이다.
            역행렬이 존재하지 않을 수도 있다.

    >**convex 함수**
        어떠한 지점에서 시작을 하더라도 최소점을 구할 수 있는 함수
        굴곡이 있는 함수는 불가능 하다
        평균 제곱 오차는 항상 convex 함수이다.



---


### 다항 회귀
>1. 다항 회귀
	데이터를 보고 알맞은 다항 함수를 구하는 것


>2. 단일 속성 다항 회귀
	기존의 다항 함수와 똑같이 하면 된다.
	일차항이 여러개인 것을 차수를 바꾸며 하면 동일하다.


>3. 다중 다항 회귀
	입력 변수가 많은데 다항 회귀를 하는 경우
	변수가 함께 곱해진 것이 가능하다.
	x1^2, x1*x2,..
	서로가 서로에게 영향을 주기 때문에 복잡한 프로그래밍이다.






---

### 로지스틱 회귀
>1. 분류 문제
	정해진 몇 개의 값 중 예측을 하는 것
	선형 회귀는 예외적인 데이터에 민감하게 분류되기 때문에 분류에 사용되지 않음


>2. 로지스틱 회귀
	데이터에 가장 잘 맞는 시그모이드 함수를 만든다.
	S(x) = 1/(1+e^(-1))
	0과 1사이의 값을 나타내기 때문에 분류를 하기 더 쉽다.
	예외적인 데이터에 민감하지 않다.
	0과 1사이의 연속적인 값이기 때문에 회귀로 표현하지만 분류로 사용한다.


>3. 로지스틱 회귀 가설 함수
	가설함수 – 특정 입력 변수를 받아서 목표 변수를 예측해주는 함수
	Gtheta(x) = theta^T*x
	일차 함수는 입력과 출력이 항상 크거나 작을 수 있기 때문에 G를 S(x)의 theta^()에  넣으면 0과 1사이의 수를 구할 수 있다.
	통과할 확률이 50%를 넘으면 통과한다고 분류
	1/(1+e^-(theta0+theta1*x))인 경우 theta0만큼 왼쪽으로 그래프를 이동, theta1을 늘리면 곡선이 조여진다.
	경계선 – 50%가 되는 부분을 찾아 밑은 0으로 위는 1로 표시
	속성이 여러 개인 경우(x1, x2) - 식을 풀어 관계식을 만들고 관계식이 50%가 되는 부분을 경계선으로 잡고 0과 1을 구분한다.


>4. 로그 손실
	손실 함수 – 데이터에 잘 맞는 가설 함수를 찾는 함수
	로지스틱 회귀에서 사용
	logloss(htheta(x)(예측값), y(실제값)) = -log(htheta(x))   y = 1
		   		          = -log(1-htheta(x)) y = 0
	y = 1인 경우 htheta(x)가 1에 가까워질수록 손실이 적다.
	손실의 정도를 로그로 결정한다.


>5. 손실 함수
	로그 손실을 이용
	logloss(htheta(x), y) = -ylog(htheta(x))-(1-y)log(1-htheta(x))
	모든 데이터의 로그 손실을 계산한 후 평균을 내면 된다.


>6. 경사 하강법
	손실을 최소화하는 하나의 방법
	선형 회귀랑 동일한 값이 나옴
	theta값을 0또는 임의로 설정
	theta를 업데이트하며 손실을 최소화


>7. 분류가 3개 이상일 때
	0, 1, 2로 표시
	0과 1,2로 분류->  1과 0,2로 분류 ->2와 0,1로 분류로 3개의 가설함수 만든다.
	입력 데이터를 각각 넣어서 0, 1, 2확률을 계산하여 가장 높은 확률인 것으로 분류


>8. 정규 방정식과 비교
	단순 행렬 연산으로 손실 함수 최소 지점 찾기 불가능
	선형식이 아니기 때문에 일차식으로 표현이 불가능하여 편미분이 안된다.
\


---

### 전처리
	머신 러닝 알고리즘의 속도와 정확도를 높이는 법
	이론이 라이브러리에서 적용하는 법
	주어진 데이터를 그대로 사용하지 않고 가공해서 모델을 학습시키는데 더 좋은 형식으로 만드는 것


>1. 입력 변수/속성 조정(feature scaling: normalization)
	입력 변수들의 크기를 조정해서 일정 범위 내에서 사용하도록 하는 것
	경사 하강법을 좀 더 빨리할 수 있도록 한다.
	min-max normalization(숫자의 크기를 0에서 1사이로 만든다)
		최솟값, 최대값을 이용하여 0에서 1사이로 만든다.
		최대와 최소의 차이를 구한다.
			모든 값에서 최솟값을 빼고 최대와 최소의 차이를 나눈다.
		일반화
			xnew: feature scaling 한 데이터
			xold: feature scaling 하기 전 데이터
			xmax: 데이터 최댓값
			xmin: 데이터 최솟값
			xnew = (xold-xmin)/(xmax-xmin)


>2. feature scaling과 경사 하강법
	큰 값부터 작은 값까지 무지개로 빨->보
	빨간점 – 최솟점(경사 하강법을 찾으려는 지점)
	가장 가파른 방향 – 등고선과 수직인 방향 *****
	세타1의 값이 조금만 바뀌어도 아웃풋이 많이 바뀐다.
	세타0은 항상 1과 곱해져서 바뀌어도 아웃풋이 많이 바뀌지 않는다
	-> 아웃풋이 많이 바뀐다는 것은 손실 함수에도 큰 영향을 준다.
	->일반: 세타1 방향으로 움직이면 손실이 급격히 변한다. 지그제그로 움직임
	->feature scaling을 하면 최솟점을 찾을 때 직선으로 와서 최단거리로
	모든 알고리즘의 속도를 빠르게 해준다.


>3. feature scaling: 표준화(standardization)
	평균(mean)
		데이터총합/데이터개수
	표준편차(standard deviation) - 시그마로 표시
		루트((데이터값-평균)의 제곱의 합/데이터개수)
	표준화
		xnew = (xold-평균)/표준편차
		xnew: 표준화한 데이터
		xold: 표준화하기 전 데이터


>4. one-hot encoding
	수치형(numerical) 데이터: 나이, 몸무게, 킴
	범주형(categorical) 데이터: 혈액형, 성별
		숫자를 지정해서 수치형 데이터로 바꾼다.
			원하지 않는 크고 작은 것이 생김->예측에 방해가 될 수도 있다. 사용X
	행렬을 만들어서 범주형 데이터를 바꾼다.
		자신의 데이터에만 1을 넣고 나머지에는 0을 넣는다.
		크고 작은 엉뚱한 관계를 만들지 않고 수치형 데이터로 바꿀 수 있다.


---
### 정규화
>1. 편향(bias), 분산(variance)
	곡선 모델이 더 정확하게 표현하다
	편향이 낮은 모델
		모델이 데이터들 사이의 관계를 완벽하게 학습할 수 있다.
	편향이 낮다고 항상 좋은 모델이 아니다.
		test 데이터가 들어올 때 오히려 오차가 더 많이 나서 성능이 안 좋을 수도 있다. 일관된 성능을 보여주는 것이 아니라 training모델을 그대로 외워서 표시를 한 것이기 때문이다.
	편향이 높은 모델
		너무 간단해서 주어진 데이터의 관계를 잘 학습하지 못한다.
		직선 모델은 너무 간단해서 복잡한 곡선 관계를 학습을 할 수 없다 
	분산 – 데이터 셋 별로 모델이 얼마나 일관된 성능을 보여주는지를 비교하는 것
		분산이 낮다 -> training 모델과 test모델의 성능이 비슷하다.
		분산이 높다 -> 성능 차이가 많이 난다.
		다양한 테스트 데이터가 주어졌을 때 모델의 성능이 얼마나 일관적인지



>2. 편향-분산 트레이드오프(bias-variance tradeoff)
	**과소적합(underfit)**
	>	복잡도가 떨어지기 때문에 곡선 관계 학습할 수 없다.
		어떤 데이터가 주어져도 일관적인 성능을 낸다.
		편향이 높고 분산이 낮은 모델이다.<br>
	**과적합(overfit)**
	>	training 데이터에 대한 성능이 아주 좋다.
		처음 보는 testing 데이터에 대한 성능은 떨어진다.
		편향이 낮고 분산이 높은 모델이다.<br>
	**->편향과 분산은 하나가 늘어나면 하나가 줄어드는 관계를 가진다.
	밸런스를 맞춰서 해결을 해야 된다.**
    


1. 정규화
	모델 과적합 현상을 방지해 주는 방법 중 하나
	함수가 급격하게 변하는 이유 - 세타 값이 굉장히 크기 때문에
	세타 값이 커지는 것을 방지한다.
	여러 데이터 셋에 일관된 성능을 유지한다. 
	머신 러닝 모델을 학습시킬 때 세타 값들이 너무 커지는 것을 방지해주는 기법
	#### L1정규화(Lasso regression)
		손실함수 + 세타 값
		세타 값이 커질수록 가설 함수가 안 좋아진다.
		정규화를 할 때 세타0은 상수항이기 때문에 빼고 한다.
		세타 값이 커지면 패널티를 주게 람다를 줄 수 있다.
			람다가 크면 세타 값이 커지면 손실 함수가 굉장히 커짐-> 세타값을 작게 설정해야 된다.
			람다가 작으면 세타 값이 커져도 손실함수가 커지지 않는다-> 평균 제곱 오차를 작게 설정해야 된다.
	#### L2정규화(Ridge regression)
		세타의 제곱을 더한다.

	#### L1, L2 정규화 차이점
		L1 정규화는 여러 세타의 값을 0으로 만들어준다. 모델에서 중요하지 않은 속성들을 모두 없애버린다.
			모델에 쓰이는 속성이나 변수의 개수를 줄이고 싶을 때 사용한다.
			a+b=t 꼭짓점에서 평균 제곱 오차항 등고선과 만날 가능성이 높다.
		L2 정규화는 세타 값들을 조금씩 줄여준다.
			속성을 줄일 필요가 없을 때 사용한다.
			a^2+b^2=t 모든 곳에서 만날 확률이 비슷함 – 축에서 안 만날수도
















---


### CNN(Convolutional Neural Networks) - 합성곱 신경망
	합성곱층(convolutional layer)와 풀링층(pooling layer)로 구성
	모델이 직접 이미지, 비디오, 텍스트 또는 사운드를 분류하는 머신 러닝의 한 유형
	딥 러닝에 가장 많이 사용되는 알고리즘
	데이터에서 직접 학습하며, 패턴을 사용하여 이미지를 분류하고 특징을 수동으로 추출할 필요가 없다.
	객체 감지에 이용 – 이미지나 비디오에서 객체를 찾아내어 분류하는 프로세스
<br>
	
    유용한 이유
		특징을 직접 학습하기 때문에 특징을 수동으로 추출해야 할 필요가 없다.
		가장 높은 수준의 인식 결과를 보인다.
		기존 네트워크를 바탕으로 한 새로운 인식 작업을 위해 재학습하여 사용 가능하다.
	작동 방식
		여러 계층이 이미지의 서로 다른 특징을 감지하도록 학습한다.
		필터는 학습 이미지에 다른 해상도로 적용되어 출력이 다음 계층의 입력으로 활용된다.
	특징 학습, 계층 및 분류
		계층 – 컨벌루션, 활성화/ReLU, 풀링
		컨벌루션 – 이미지에서 특정 특징을 활성화하는 필터 집합에 입력 이미지를 통과 시킨다.
		ReLU(Rectified Linear Unit) - 음수 값을 0으로 양수 값을 유지하여 빠르고 효과적으로 학습한다. 활성화된 특징만 다음 계층으로 전달된다.
		풀링 – 비선형 다운샘플링을 수행하고 네트워크에서 학습해야 하는 매개 변수 수를 줄여 출력을 간소화한다.
	합성곱층
		완전 연결 계층(픽셀의 순서대로 저장이 되어 데이터의 형상을 무시하는 일차원 데이터로 입력을 받아 공간적 구조를 무시한다.)와 다르게 입력 데이터의 형상을 유지한다.
		출력 또한 3차원 데이터로 다음 계층으로 전달한다.
		데이터를 제대로 학습할 수 있다.
		수용영역에 있는 픽셀만 연결이 된다.
	필터
		합성곱층에서의 수용 영역
		가중치 파라미터(W)로 계산을 한다.
		필터와 입력 데이터를 이용하여 특성맵(feature map)을 출력하여 다음 층으로 전달한다.
	합성곱층 연산
		데이터와 필터를 (높이, 너비)로 표시한다. - 윈도우
		필터의 윈도우를 일정한 간격으로 이동하며 계산한다.
		입력 데이터와 필터에 대응하는 원소끼리 곱한 후 총합을 구한다.
		편향(bias)은 필터를 적용한 후에 더한다.
	패딩(padding)
		합성곱 연산을 수행하기 전, 입력 데이터 주변을 특정값으로 채워 늘리는 것
		출력 데이터의 공간적 크기를 조절하기 위해 사용한다.
		zero-padding 사용 – 0으로 모두 채워 넣는다.
		사용하는 이유 – 가장자리 정보가 사라지는 문제가 발생하기 때문에 출력이 입력 데이터와 공간적 크기와 동일하게 맞추기 위해서 사용한다.
	스트라이드(stride)
		입력 데이터에 필터를 적용할 때 필터가 이동하는 간격을 조절한다.
	출력 크기 계산
		(OH, OW) = ((H+2P-FH)/S +1, (W+2P-FW)/S +1)
		(H, W) = input size
		(FH, FW) = filter/kernel size
		S = stride
		P = padding
		(OH, OW) = output size
	3차원 데이터의 합성곱
		입력 데이터의 채널수와 필터의 채널수가 같아야 한다.
		(높이, 너비, 채널)로 표현
	풀링층(pooling layer)
		model
			max-pooling – pooling의 윈도우 사이즈에서 가장 큰 값을 넣는다. 이미지 인식 분야에서 사용한다.
			average-pooling – pooling의 윈도우 사이즈에서 평균을 내서 넣는다.
		계산된 특징이 이미지 내의 위치에 대한 변화에 영향을 덜 받는다.
		불변성을 찾아내서 공간적 변화를 극복할 수 있다.


https://excelsior-cjh.tistory.com/180 텐서플로 확인하기


---
### Perceptron – 퍼셉트론

	다수의 신호를 입력 받아서 하나의 신호로 출력한다.
	입력신호에 가중치를 주어 계산하고 신호의 총합이 정해진 임계값을 넘으면 1을 출력하고 넘지 못하면 0 또는 –1을 출력한다.
	입력 신호에 고유 weight가 부여되어 클수록 중요하다.
<br>

	용어
		임계치(threshold) - 값이 활성화되기 위한 최솟값
		가중치(weight) - 선형 분류를 위한 경계를 찾는 것, 경계의 방향성과 형태
		바이어스(bias) - 선형 경계의 절편을 나타내는 값, 직선은 y절편
		net값 – 입력값과 가중치의 곱을 모두 합한 값
		활성함수(activation function) - net값이 임계치보다 크면 1을 표시, 단층 퍼셉트론에서만 유효
		뉴런(neuron) - 활성 함수와 비슷, 임계치보다 크면 활성화 시키며 1을 출력 작으면 비활성화를 시키며 0을 출력
	입력층과 출력층(임의의 n개의 뉴런으로 구성)으로 구성된다.
<br>

	학습 방법
		임의로 설정된 weight로 시작한다.
		분류가 잘못되었을 때 weight를 개선해 나간다.
		학습 데이터가 선형적으로 분리될 수 있을 때 적합하다.
<br>

	가중치와 편향
		편향은 input의 가중치가 계산되어 넘어야 하는 임계점이다.
			높을수록 분류의 기준이 엄격하고 모델이 간단해지는 경향이 있다.
				과소적합의 위험이 발생하게 된다.
			낮을수록 데이터의 허용범위가 넓어지며 학습 데이터에 잘 맞는 모델이 만들어지고 복잡해진다.
				필요 없는 노이즈가 발생할 가능성이 높다.
		가중치 – 입력신호가 결과 출력에 주는 영향도를 조절하는 매개변수
		편향 – 쉽게 활성화(1로 출력)가 되는 것을 조정하는 매개변수
<br>

	한계점
		XOR논리와 같은 비선형 분류는 불가능
<br>

	극복 방법
		다층 퍼셉트론 사용(MLP)
<br>

	알고리즘
		가중치와 바이어스 가중치를 –0.5에서 0.5사이의 임의의 값으로, 입력값을 임의의 값으로 초기화
		하나의 학습 벡터에 대한 출력층 뉴런의 net값을 계산
		활성 함수를 통해 계산된 net값으로부터 실제 출력값을 계산
		목표값과의 차이가 허용 오차보다 크면 학습을 진행
		마지막 학습 벡터까지 반복
		마지막 벡터의 오차가 크면 학습 벡터를 처음부터 반복
---
### MLP(multi-layer perceptron) - 다층 퍼셉트론
	퍼셉트론으로 이루어진 층 여러개를 순차적으로 붙여놓은 형태
	같은 층끼리는 연결이 되어있지 않다.
	다시 연결되는 feedback이 되지 않는다.
	한 방향으로만 이동한다.
	각 층은 하나의 노드로 동작
	층의 개수(depth)와 크기(width)로 결정
	비선형적으로 분리되는 데이터를 이용할 때 용이
	은닉층 – 입력층과 출력층 사이에 존재하는 중간층
	심층 신경막(deep neural network) - 여러개의 은닉층이 있는 인공 신경막
	딥 러닝 – 심층 신경망을 학습하기 위해 고안된 알고리즘
	순전파(feedforward) - 앞의 값들이 모든 노드로 전달
	입력층: x0, x1 ...(n개) 은닉층: a0^(2), a1(2) ...(m개) 출력층: a1(3), a2(3) ...(k개)
	-> n-m-k 다층 퍼셉트론, x0, a0은 1로 두고 실행
<br>

	동작 원리
		가중치를 임의의 값으로 설정, 바이어스는 1로 설정
		하나의 데이터에서 순입력 함수값 계산하여 최종 활성 함수의 출력값 계산
		실제값과 결과값의 허용 오차 이내가 되도록 각층의 가중치를 업데이트
		허용 오차 이내가 되면 학습을 종료
<br>

	문제점
		은닉층의 출력값에 대한 기준값을 정의할 수 없다.
		은닉층의 정답의 기준이 없다.
<br>

	해결 방안
		출력층에서 발생하는 오차값을 이용하여 은닉층으로 연전파(backpropagation)하여 은닉층에서의 오차값에 따라 은닉층의 가중치를 업데이트
	





https://ko.d2l.ai/chapter_crashcourse/introduction.html 딥러닝 블로그








```
minibatch – 데이터 개수 중 몇 개를 뽑아서 데이터로 학습을 시키고 비교를 할 수 있도록 하는 것 batchsize – 임의로 추출한 데이터 개수, batch – 추출한 데이터로 나온 결과
cross-entropy - -(ylog(yp+작은 상수)) 0에서 1사이의 값으로 나온 예측 값을 결과와 비교하는 것
drop out – 과접합이 일어날 때 몇 가지를 빼서 덜 굴곡이 있도록 만들어서 과접합이 안 일어나도록 만드는 것
affine – input을 받을 때 함수가 계산되는 과정까지
active function – step function, ReLu, sigmoid를 하는 것
hidden layer – affine + active function
softmax – active function의 일부로 output으로 이동하기 전에 사용한다.
	output = (affine, A.F한 뒤값)*W한 뒤 값/전체의 값
epoch(에펙) – proceptron에서 시작하여 모든 과정을 한 바퀴 도는 작업
	순서
	proceptron
	MLP
	신경망 – input(data 전처리 -> one hot encoding)
		hidden(affine + A.F) 반복 -> 최종 affine + softmax
		output
	loss function – MSE(평균제곱오차), cross-Entropy error(선형회귀
costfunction-lossfunction)
	chainrule – back propagation(역전파)
	minibatch, dropout

https://colab.research.google.com/drive/16fSbCyLntAUWtHE0bfGG8XLOq16Ut-l_#scrollTo=O930tS6t_X-G 코랩 코드
tf.keras.layers.Flatten() -1x1행렬로 펼치는 것
```